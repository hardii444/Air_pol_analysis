# -*- coding: utf-8 -*-
"""air_pol_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11B4z-3NqsGuoHXslFRxVL6gI0egCxcn0
"""

# Import useful libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import geopandas as gpd

import pandas as pd
from google.colab import files

df = pd.read_csv("/content/data.csv", encoding = "ISO-8859-1")
df.head()

df['date'] = pd.to_datetime(df['date'],format='%Y-%m-%d') # date parse
df['year'] = df['date'].dt.year # year
df['year'] = df['year'].fillna(0.0).astype(int)
df = df[df['year']>0]

df.shape

df.dtypes.value_counts()

df.isnull().sum()

def printNullValues(df):
    """
    Prints the number of null values for each column in the dataframe.
    """
    for col in df.columns:
        null_count = df[col].isnull().sum()
        if null_count > 0:
            print(f"Column '{col}' has {null_count} null values.")


# Now you can call the function:
printNullValues(df)

df["type"].value_counts()

sns.catplot(x = "type", kind = "count",  data = df, height=5, aspect = 4)

# Convert the 'so2' column to numeric, coercing errors to NaN
df['so2'] = pd.to_numeric(df['so2'], errors='coerce')

# Now calculate the mean after type conversion
# Filter out non-numeric values before calculating the mean
grp = df.groupby(["type"])['so2'].apply(lambda x: pd.to_numeric(x, errors='coerce').mean()).to_frame()
grp.plot.bar(figsize = (20,10))

# Convert the 'no2' column to numeric, coercing errors to NaN
df['no2'] = pd.to_numeric(df['no2'], errors='coerce')

# Now you can calculate the mean after type conversion
grp = df.groupby(["type"])['no2'].mean().to_frame()

grp.plot.bar(figsize = (20,10))

df[['so2', 'state']].groupby(['state']).median().sort_values("so2", ascending = False).plot.bar(figsize=(20,10))

df[['so2', 'year', 'state']].groupby('year')[['so2']].median().sort_values(by='year', ascending=False).plot(figsize=(20, 10))

df[['no2', 'state']].groupby(['state']).median().sort_values("no2", ascending = False).plot.bar(figsize=(20,10))

df[['no2', 'year', 'state']].groupby("year")[['no2']].median().sort_values(by='year', ascending=False).plot(figsize=(20, 10))

df[['spm', 'state']].groupby(['state']).median().sort_values("spm", ascending = False).plot.bar(figsize=(20,10))

df[['spm','year','state']].groupby(["year"])[['spm']].median().sort_values(by='year',ascending=False).plot(figsize=(20,10))

fig, ax = plt.subplots(figsize=(20,10))
sns.heatmap(df.pivot_table('so2', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=True, linewidths=.5)

fig, ax = plt.subplots(figsize=(20,10))
sns.heatmap(df.pivot_table('no2', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=True, linewidths=.5)

fig, ax = plt.subplots(figsize=(20,10))
sns.heatmap(df.pivot_table('spm', index='state',columns=['year'],aggfunc='median',margins=True),ax = ax,annot=False, linewidths=.5)

import matplotlib.pyplot as plt

# Group by 'year' and 'state', then calculate the median of 'no2'
df_grouped = df.groupby(['year', 'state'])['no2'].median().unstack()

# Plot the data (Year on x-axis and median 'no2' on y-axis, states as separate lines)
df_grouped.plot(figsize=(20, 10))

# Display the plot
plt.title("Median NO2 by Year and State")
plt.xlabel("Year")
plt.ylabel("Median NO2")
plt.legend(title="State")
plt.show()

temp = df.pivot_table('spm', index='year',columns=['state'],aggfunc='median',margins=True).reset_index()
temp = temp.drop("All", axis = 1)
temp = temp.set_index("year")
temp.plot(figsize=(20,10))

temp = df.pivot_table('so2', index='year',columns=['state'],aggfunc='median',margins=True).reset_index()
temp = temp.drop("All", axis = 1)
temp = temp.set_index("year")
temp.plot(figsize=(20,10))

upload = files.upload()

!pip install sentinelsat
!pip install geopandas
!pip install folium

import geopandas as gpd
import folium
from sentinelsat import SentinelAPI

india = gpd.read_file(r"Indian_States.shp")
india.info()

india.plot()

# Matching the names of states between the two datasets

india["st_nm"] = india["st_nm"].apply(lambda x: x.lower())
india = india.set_index("st_nm")
df["state"] = df["state"].apply(lambda x: x.lower())

# Filter the data for years before 2001
df_before_2001 = df[df["year"] < 2001]

# Keep the 'state' column and select only numeric columns
df_before_2001_numeric = df_before_2001[['state'] + df_before_2001.select_dtypes(include=['number']).columns.tolist()]

# Group by 'state' and calculate the mean for numeric columns
df_before_2001_grouped = df_before_2001_numeric.groupby("state").mean()

# Display the result
df_before_2001_grouped.head()

# Filter the data for years after 2001
df_after_2001 = df[df["year"] >= 2001]

# Keep the 'state' column and select only numeric columns
df_after_2001_numeric = df_after_2001[['state'] + df_after_2001.select_dtypes(include=['number']).columns.tolist()]

# Group by 'state' and calculate the mean for numeric columns
df_after_2001 = df_after_2001_numeric.groupby("state").mean()

import geopandas as gpd

# Assuming 'india' is your GeoDataFrame, replace 'india' with your actual GeoDataFrame name if different
result = india.copy()  # Create a copy of the GeoDataFrame and assign it to 'result'

# Now you can inspect and filter
print(result["geometry"].head())  # Inspect geometry data
crs = "EPSG:4326"  # Define Coordinate Reference System
result = result[result["geometry"].notna()]  # Filter out rows with missing geometries

# Ensure all geometries are valid within the 'result' GeoDataFrame
result = result[result.is_valid]

# If 'gdf' is intended to be the same as 'result', assign it here:
gdf = result.copy()

result = pd.concat([df_before_2001, india], axis=1, sort=False)
result = result [result["geometry"] != None]

from geopandas import GeoDataFrame
crs = {'init': 'epsg:4326'}
gdf = GeoDataFrame(result, crs=crs, geometry=result ["geometry"])
gdf['centroid'] = gdf.geometry.centroid
fig,ax = plt.subplots(figsize=(20,10))
gdf.plot(column='so2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)
plt.title("Mean So2 before 2001")
plt.axis('off')

for x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):
    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords="offset points",color='gray')

result = pd.concat([df_after_2001, india], axis=1, sort=False)
result = result [result["geometry"] != None]

from geopandas import GeoDataFrame
crs = {'init': 'epsg:4326'}
gdf = GeoDataFrame(result, crs=crs, geometry=result ["geometry"])
gdf['centroid'] = gdf.geometry.centroid
fig,ax = plt.subplots(figsize=(20,10))
gdf.plot(column='so2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)
plt.title("Mean So2 after 2001")
plt.axis('off')

for x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):
    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords="offset points",color='gray')

result = pd.concat([df_before_2001, india], axis=1, sort=False)
result = result [result["geometry"] != None]

from geopandas import GeoDataFrame
crs = {'init': 'epsg:4326'}
gdf = GeoDataFrame(result, crs=crs, geometry=result ["geometry"])
gdf['centroid'] = gdf.geometry.centroid
fig,ax = plt.subplots(figsize=(20,10))
gdf.plot(column='no2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)
plt.title("Mean NO2 before 2001")
plt.axis('off')

for x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):
    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords="offset points",color='gray')

result = pd.concat([df_after_2001, india], axis=1, sort=False)
result = result [result["geometry"] != None]


from geopandas import GeoDataFrame
crs = {'init': 'epsg:4326'}
gdf = GeoDataFrame(result, crs=crs, geometry=result ["geometry"])
gdf['centroid'] = gdf.geometry.centroid
fig,ax = plt.subplots(figsize=(20,10))
gdf.plot(column='no2',ax=ax,alpha=0.4,edgecolor='black',cmap='cool', legend=True)
plt.title("Mean NO2 after 2001")
plt.axis('off')

for x, y, label in zip(gdf.centroid.x, gdf.centroid.y, gdf.index):
    ax.annotate(label, xy=(x, y), xytext=(3,3), textcoords="offset points",color='gray')

uploaded1 = files.upload()

AQIdf = pd.read_csv("meanAQI.csv")
AQIdf['date'] = pd.to_datetime(AQIdf['date'],format='%Y-%m-%d') # date parse
AQIdf.head()

AQIdf.isna().sum()

AQIdf=AQIdf.set_index('date')
AQIdf.reset_index()
AQIdf.resample(rule="M")
AQIdf.head()

AQIdf.index

AQIdf.plot(figsize = (20,10))

AQIdf["EWMA_8"] = AQIdf["AQI"].ewm(span=8).mean()

AQIdf.plot(figsize = (25,10))

from statsmodels.tsa.seasonal import seasonal_decompose
result = seasonal_decompose(AQIdf["AQI"], model = "multiplicative",period=8)

fig = result.plot()

# Assuming 'df' is your DataFrame and it contains columns 'state', 'no2', 'spm', 'pm_2', and 'year'.

import matplotlib.pyplot as plt

# Group data by state and year, then calculate the mean of 'no2', 'spm', and 'pm_2'
pollutants = ['no2', 'spm', 'pm2_5']  # Corrected column name to pm2_5

for pollutant in pollutants:
  # Check if the pollutant column exists
  if pollutant in df.columns:
    grouped_data = df.groupby(['year', 'state'])[pollutant].median().unstack()

    # Plot the data
    plt.figure(figsize=(200, 100))  # Adjust the figure size
    grouped_data.plot()
    plt.title(f"Median {pollutant.upper()} by Year and State")
    plt.xlabel("Year")
    plt.ylabel(f"Median {pollutant.upper()}")
    plt.legend(title="State")
    plt.grid(True)  # Add grid for better readability
    plt.show()
  else:
      print(f"Warning: Column '{pollutant}' not found in the DataFrame.")

import multiprocessing as mp
from multiprocessing import Pool
import time

pool = mp.Pool(mp.cpu_count())

df.shape

from statsmodels.tsa.stattools import adfuller
def ADF_Test(df):
    result = adfuller(df)
    print('Augmented Dickey-Fuller Test:')
    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']
    for value,label in zip(result,labels):
        print(label+' : '+str(value) )
    if result[1] <= 0.05:
        print("strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary")
    else:
        print("weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary ")

import time

df

import random

ADF_Test(AQIdf['AQI'])

AQIdf['AQI_First_Diff']=AQIdf['AQI']-AQIdf['AQI'].shift(8)
AQIdf=AQIdf.dropna()
AQIdf.head()

ADF_Test(AQIdf['AQI_First_Diff'])

AQIdf["AQI_First_Diff"].plot(figsize = (20,10))

from pandas.plotting import autocorrelation_plot
autocorrelation_plot(AQIdf['AQI'])

from statsmodels.graphics.tsaplots import plot_acf,plot_pacf
## p(number of months till shut-off)=3, d(how many times difference)=1, q(exponential decrease till)=2

fig = plt.figure(figsize=(15,10))
ax1 = fig.add_subplot(211)
fig = plot_acf(AQIdf["AQI_First_Diff"],lags=50,ax=ax1)
ax2 = fig.add_subplot(212)
fig = plot_pacf(AQIdf["AQI_First_Diff"],lags=50,ax=ax2)

from statsmodels.tsa.arima.model import ARIMA

AQIdf.index = pd.DatetimeIndex(AQIdf.index).to_period('M')

model=ARIMA(AQIdf['AQI'],order=(3,1,2))
model_fit=model.fit()

print(model_fit.summary())
model_fit.resid.plot()

model_fit.resid.plot(kind='kde')

AQIdf['forecast'] = model_fit.predict(start = 285, end= 338, dynamic= True)
AQIdf[['AQI','forecast']].plot(figsize=(20,10))

import statsmodels.api as sm

seasonal_model = sm.tsa.statespace.SARIMAX(AQIdf["AQI"],order=(3,1,2), seasonal_order=(3,1,2,8))
results = seasonal_model.fit()
print(results.summary())
results.resid.plot()

results.resid.plot(kind='kde')

AQIdf['Seasonal_forecast'] = results.predict(start = 284, end= 338, dynamic= True)
AQIdf[['AQI','Seasonal_forecast']].plot(figsize=(20,10))

AQIdf.index=AQIdf.index.to_timestamp() # date parse
AQIdf.reset_index()
AQIdf.resample(rule="M")
AQIdf.head()

AQIdf.index.map(lambda t: t.replace(day=1))

from pandas.tseries.offsets import DateOffset
future_dates = [AQIdf.index[-1] + DateOffset(months=x) for x in range(0,63)]
future_AQIdf = pd.DataFrame(index=future_dates[1:],columns=AQIdf.columns)
Combined_AQIdf = pd.concat([AQIdf,future_AQIdf])
Combined_AQIdf.index = pd.DatetimeIndex(Combined_AQIdf.index).to_period('M')
Combined_AQIdf['AQI_Forecast'] = results.predict(start = 338, end = 400, dynamic= True)
Combined_AQIdf[['AQI', 'AQI_Forecast']].plot(figsize=(20, 10))